# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_lFrI4xKg_1-DYxhSL6soBujwGmdVokJ
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
 
import keras
from keras.models import Sequential
from keras.layers import InputLayer
from keras.layers import Dense
from keras.layers import Dropout
from keras.constraints import maxnorm

df = pd.read_csv('churn.csv')

df.head(5)

pip install -U scikit-learn==0.20

dataset = pd.read_csv('Churn_Modelling.csv')
#Looking at the features we can see that row no.,name will have no relation with a customer with leaving the bank
#so we drop them from X which contains the features Indexes from 3 to 12
X = dataset.iloc[:, 3:13].values
#We store the Dependent value/predicted value in y by storing the 13th index in the variable y
y = dataset.iloc[:, 13].values
#Printing out the values of X --> Which contains the features
#                           y --> Which contains the target variable
print(X)
print(y)

# Encoding the Independent Variables
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

labelencoder_X_1 = LabelEncoder()#creating label encoder object no. 1 to encode region name(index 1 in features)

X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])#encoding region from string to just 3 no.s 0,1,2 respectively

labelencoder_X_2 = LabelEncoder()#creating label encoder object no. 2 to encode Gender name(index 2 in features)

X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])#encoding Gender from string to just 2 no.s 0,1(male,female) respectively

#Now creating Dummy variables
onehotencoder = OneHotEncoder(categorical_features = [1])
X = onehotencoder.fit_transform(X).toarray()
X = X[:, 1:]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

import keras
from keras.models import Sequential#For building the Neural Network layer by layer
from keras.layers import Dense#To randomly initialize the weights to small numbers close to 0(But not 0)

# Initialising the ANN
#So there are actually 2 ways of initializing a deep learning model
#------1)Defining each layer one by one
#------2)Defining a Graph
classifier = Sequential()#We did not put any parameter in the Sequential object as we will be defining the Layers manually

# Adding the input layer and the first hidden layer
#This remains an unanswered question till date that how many nodes of the hidden layer do we actually need
# There is no thumb rule but you can set the number of nodes in Hidden Layers as an Average of the number of Nodes in Input and Output Layer Respectively.
#Here avg= (11+1)/2==>6 So set Output Dim=6
#Init will initialize the Hidden Layer weights uniformly
#Activation Function is Rectifier Activation Function
#Input dim tells us the number of nodes in the Input Layer.This is done only once and wont be specified in further layers.
classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))

# Adding the output layer
classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))
#Sigmoid activation function is used whenever we need Probabilities of 2 categories or less(Similar to Logistic Regression)
#Switch to Softmax when the dependent variable has more than 2 categories

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)

y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)#if y_pred is larger than 0.5 it returns true(1) else false(2)
print(y_pred)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy=(1546+134)/2000#Obtained from Confusion Matrix
print(accuracy)

from sklearn.metrics import accuracy_score 
accuracy_score(y_test, y_pred)

